{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOotJZIL2FWYYPiLud4KZuV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinaya1204/FMML_2023/blob/Labs/Module%207%20Lab%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Means Clustering**"
      ],
      "metadata": {
        "id": "k9yA179oIHOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)** **Is feature scaling essential for KMeans as it is for most ML algos?**\n"
      ],
      "metadata": {
        "id": "C2Oqw3-gD-15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, because KMeans clustering is sensitive to feature scale, feature scaling is usually necessary. Distances between data points are used by the KMeans algorithm to group the data into clusters. The algorithm might be more swayed by characteristics with bigger scales if the features have varied scales, which could result in skewed cluster assignments.\n",
        "\n",
        "Feature scaling guarantees that each feature makes an equal contribution to the distance calculations and, as a result, to the clustering procedure. Standardisation (Z-score scaling) or Min-Max scaling (normalisation) are two popular techniques for feature scaling. While standardisation changes the data to have a mean of 0 and a standard deviation of 1, it also adjusts the features to a defined range, typically between 0 and 1.\n",
        "\n",
        "\n",
        "The performance and efficiency of the clustering technique are enhanced by applying feature scaling prior to executing KMeans. This helps build a more accurate representation of the underlying patterns in the data."
      ],
      "metadata": {
        "id": "549lt2fMEBZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) How can we prevent initialization variation in KMeans?**"
      ],
      "metadata": {
        "id": "4KSfRDAXEyvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sensitivity of the algorithm to the initial positioning of cluster centroids is known as initialization variation in KMeans. Variations in the initialization can result in variations in the final cluster allocations and, occasionally, less than ideal outcomes. Numerous methods to increase KMeans' resilience have been put forth in an effort to address this problem. The following techniques can be used to stop initialization variation:\n",
        "\n",
        "**Multiple Initializations:** Execute the KMeans method several times using various initializations, then select the outcome that yields the lowest inertia (total sum of squared distances) or other clustering evaluation metric. For instance, the n_init option in Scikit-learn's KMeans implementation regulates how many times the algorithm is executed using various centroid seeds.\n",
        "\n"
      ],
      "metadata": {
        "id": "edSC7yrOFE_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)"
      ],
      "metadata": {
        "id": "Jy4ypSG8Fr6s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KMeans++ Initialization:**KMeans++ is a better initialization method than random ones. It usually disperses the initial centroids over the dataset more efficiently, decreasing the probability of reaching a less-than-ideal solution. This approach is frequently used by default in numerous libraries, such as scikit-learn."
      ],
      "metadata": {
        "id": "loKOAoy1F6eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n"
      ],
      "metadata": {
        "id": "NOSCo0uwF-7c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mini-Batch KMeans:**A KMeans variant known as Mini-Batch KMeans modifies the cluster centroids at each iteration by using a random portion of the data. There are instances when this is less dependent on where the centroids were first placed."
      ],
      "metadata": {
        "id": "ZIZgYnKEGEnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "kmeans = MiniBatchKMeans(n_clusters=3, random_state=42)\n"
      ],
      "metadata": {
        "id": "8zs97saZGPSX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using these methods, you may improve the stability of the KMeans algorithm and lessen the effect of initialization variance. To identify more reliable and representative cluster assignments, try varying the initialization strategy and running the process several times using different seeds."
      ],
      "metadata": {
        "id": "35u36NwvGY-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) What is the training and testing complexity of KMeans?**"
      ],
      "metadata": {
        "id": "qGW_vAIXGaOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KMeans clustering algorithm's training and testing complexity are determined by the quantity of data points (N), the number of clusters\n",
        "(d), the quantity of features (k), and the convergence requirements. While the testing complexity usually entails allocating new data points to preexisting clusters, the training complexity is the computing expense incurred during the clustering process.\n",
        "\n",
        "**Training Complexity:**\n",
        "\n",
        "\n",
        "*  **Worst-case time complexity:**I is the number of iterations, and O(N⋅k⋅d⋅I⋅t).The number of centroids to be updated in each iteration is denoted by t.\n",
        "\n",
        "*  **Typical time complexity:** The time complexity may be lower in reality, as the algorithm frequently converges in a modest number of iterations. The temporal complexity can also be increased by the initialization process, which is usually KMeans++ initialization.\n",
        "\n",
        "\n",
        "**Testing (Prediction) Complexity:**\n",
        "The time complexity of locating the closest cluster centroid for a new data point is O(k⋅d), where d is the number of features and k is the number of clusters.\n",
        "\n",
        "It's crucial to remember that the convergence criterion, the implementation specifics, and the initialization approach selected can all have an impact on the time complexity of KMeans. For large datasets, Mini-Batch KMeans is a variation that can be more computationally efficient as it employs random portions of the data at each iteration.\n",
        "When it comes to space complexity, KMeans usually need ￰(\n",
        "\n",
        "O(N⋅d) is used to store the dataset, and O(k⋅d) is used to store the cluster centroids.\n",
        "\n",
        "Remember that while these complexity offer a broad overview, the actual performance may vary depending on hardware, individual implementations, and optimisation strategies applied to a given library or framework.\n",
        "\n"
      ],
      "metadata": {
        "id": "hmZbzE0xGfTm"
      }
    }
  ]
}